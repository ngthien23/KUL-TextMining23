{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4591937",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36cfcf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import io \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de3aa81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54406"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path = r\"C:\\Users\\alice\\Desktop\\UNIBO\\DATA SCIENCE\\SECOND YEAR\\TEXT MINING\\Project1\\Scripts TBBT.csv\"\n",
    "df = pd.read_csv(\"Scripts TBBT.csv\", sep = \",\", skipinitialspace=True, engine=\"python\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceafa5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "leonard_df = df[df['person_scene'] == 'Leonard'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928d0fb",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "On average, how many sentences and words does your character have to speak per\n",
    "episode? Does this deviate across seasons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fcb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentences in each dialogue (row)\n",
    "leonard_df['num_sentences'] = leonard_df['dialogue'].apply(lambda x: len(sent_tokenize(str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "156e21dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_name</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>person_scene</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>dialogue_no_punct</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>Agreed, what’s your point?</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>1</td>\n",
       "      <td>agreed what s your point</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>Excuse me?</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>1</td>\n",
       "      <td>excuse me</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>One across is Aegean, eight down is Nabakov, ...</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>3</td>\n",
       "      <td>one across is aegean eight down is nabakov tw...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>Yes. Um, is this the High IQ sperm bank?</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>2</td>\n",
       "      <td>yes um is this the high iq sperm bank</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>Thank-you. We’ll be right back.</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>2</td>\n",
       "      <td>thank you we ll be right back</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            episode_name  \\\n",
       "2   Series 01 Episode 01 – Pilot Episode   \n",
       "4   Series 01 Episode 01 – Pilot Episode   \n",
       "6   Series 01 Episode 01 – Pilot Episode   \n",
       "8   Series 01 Episode 01 – Pilot Episode   \n",
       "12  Series 01 Episode 01 – Pilot Episode   \n",
       "\n",
       "                                             dialogue person_scene  \\\n",
       "2                          Agreed, what’s your point?      Leonard   \n",
       "4                                          Excuse me?      Leonard   \n",
       "6    One across is Aegean, eight down is Nabakov, ...      Leonard   \n",
       "8            Yes. Um, is this the High IQ sperm bank?      Leonard   \n",
       "12                    Thank-you. We’ll be right back.      Leonard   \n",
       "\n",
       "    num_sentences                                  dialogue_no_punct  \\\n",
       "2               1                           agreed what s your point   \n",
       "4               1                                          excuse me   \n",
       "6               3   one across is aegean eight down is nabakov tw...   \n",
       "8               2              yes um is this the high iq sperm bank   \n",
       "12              2                      thank you we ll be right back   \n",
       "\n",
       "    num_words  \n",
       "2           5  \n",
       "4           2  \n",
       "6          39  \n",
       "8           9  \n",
       "12          7  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation and lowercase function\n",
    "def remove_punctuation(text):\n",
    "    # Replace specific punctuation marks with spaces\n",
    "    text = text.replace(\"\\u2019\", ' ') # Unicode for apostrophe\n",
    "    text = text.replace(\"\\u2013\", ' ')  # Unicode for en dash\n",
    "    text = text.replace(\"\\u2014\", ' ')  # Unicode for em dash\n",
    "    text = text.replace(\"-\", ' ')\n",
    "    text = text.replace(\"\\u2026\", ' ')  # Unicode for ellipsis\n",
    "    # Remove remaining punctuation marks\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    return text.lower()\n",
    "\n",
    "# Remove punctuation and lowercase dialogues\n",
    "leonard_df['dialogue_no_punct'] = leonard_df['dialogue'].apply(remove_punctuation)\n",
    "\n",
    "# Number of words per dialogue (row)\n",
    "leonard_df['num_words'] = leonard_df['dialogue_no_punct'].apply(lambda x: len(word_tokenize(str(x))))\n",
    "leonard_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082db9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences per episode\n",
    "sentences_per_episode = leonard_df.groupby('episode_name')['num_sentences'].sum().reset_index()\n",
    "\n",
    "# Average sentences per episode\n",
    "avg_sentences_per_episode = sentences_per_episode['num_sentences'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "552c31d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words per episode\n",
    "words_per_episode = leonard_df.groupby('episode_name')['num_words'].sum().reset_index()\n",
    "\n",
    "# Average words per episode\n",
    "avg_words_per_episode = words_per_episode['num_words'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb7ab4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "leonard_df['season_nr'] = leonard_df['episode_name'].str.extract(r'Series (\\d+)')\n",
    "leonard_df['season_nr'] = leonard_df['season_nr'].astype(float)\n",
    "\n",
    "sentences_per_season_episode = leonard_df.groupby(['season_nr', 'episode_name'])['num_sentences'].sum().reset_index()\n",
    "words_per_season_episode = leonard_df.groupby(['season_nr', 'episode_name'])['num_words'].sum().reset_index()\n",
    "\n",
    "# Average sentences per season-episode\n",
    "avg_sentences_per_season = sentences_per_season_episode.groupby('season_nr')['num_sentences'].mean().reset_index()\n",
    "\n",
    "# Average words per season-episode\n",
    "avg_words_per_season = words_per_season_episode.groupby('season_nr')['num_words'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "035a1e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Leonard's sentences per episode: 62\n",
      "Average Leonard's sentences per season-episode:    season_nr  num_sentences\n",
      "0        1.0           92.0\n",
      "1        2.0           79.0\n",
      "2        3.0           76.0\n",
      "3        4.0           67.0\n",
      "4        5.0           61.0\n",
      "5        6.0           53.0\n",
      "6        7.0           54.0\n",
      "7        8.0           54.0\n",
      "8        9.0           50.0\n",
      "9       10.0           44.0\n",
      "Average Leonard's words per episode: 436\n",
      "Average Leonard's words per season-episode:    season_nr  num_words\n",
      "0        1.0      713.0\n",
      "1        2.0      543.0\n",
      "2        3.0      509.0\n",
      "3        4.0      422.0\n",
      "4        5.0      420.0\n",
      "5        6.0      367.0\n",
      "6        7.0      389.0\n",
      "7        8.0      396.0\n",
      "8        9.0      352.0\n",
      "9       10.0      333.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Leonard's sentences per episode:\", round(avg_sentences_per_episode))\n",
    "print(\"Average Leonard's sentences per season-episode:\", round(avg_sentences_per_season))\n",
    "print(\"Average Leonard's words per episode:\", round(avg_words_per_episode))\n",
    "print(\"Average Leonard's words per season-episode:\", round(avg_words_per_season))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae12d06",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Globally, over all episodes within the first 10 seasons, how many times does your\n",
    "character mention nouns, and person names? Make a Wordcloud of this tag/entity to\n",
    "have a clear visualization which nouns/person names are mostly used by your character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6acf157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize \n",
    "leonard_df['tokens'] = leonard_df['dialogue_no_punct'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9b3d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    " \n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "leonard_df['tokens_no_stop'] = leonard_df['tokens'].apply(remove_stopwords) \n",
    "leonard_df['dialogue_no_stop'] = leonard_df['tokens_no_stop'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9a1f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0ad5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(tokens):\n",
    "    entities = {'nouns': [], 'persons': []}\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN' and token.ent_type_ != 'PERSON':\n",
    "            entities['nouns'].append(token.lemma_)\n",
    "        elif token.ent_type_ == 'PERSON':\n",
    "            entities['persons'].append(token.text)\n",
    "    return entities\n",
    "\n",
    "leonard_df['entities'] = leonard_df['tokens_no_stop'].apply(extract_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2d03b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the lists of nouns and persons\n",
    "all_nouns = [noun for sublist in leonard_df['entities'].apply(lambda x: x['nouns']) for noun in sublist]\n",
    "all_persons = [person for sublist in leonard_df['entities'].apply(lambda x: x['persons']) for person in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca1d02",
   "metadata": {},
   "source": [
    "This one is not perfect but works definitely better.  \n",
    "I manually remove some wrongly classified words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85f97438",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nouns = [\"sheldon\", \"penny\", \"psst\", \"co\", \"lot\", \"talk\", \"call\", \"know\"]\n",
    "all_nouns = [noun for noun in all_nouns if noun not in not_nouns]\n",
    "not_persons = [\"ho\", \"yo\", \"mm\", \"come\", \"want\", \"little\", \"need\", \"relax\", \"reason\", \"long\", \"make\", \"hey\", \"tell\", \"talking\", \"knoks\", \"hang\", \"sorry\", \n",
    "               \"listen\", \"hmm\", \"cousin\", \"sister\", \"cool\", \"bisexual\", \"hell\", \"kinda\", \"rider\", \"head\", \"bag\", \"mmm\", \"hee\", \"talk\", \"cinnamon\", \"night\",\n",
    "              \"right\", \"bad\", \"fine\", \"lesbian\", \"know\", \"moo\", \"boy\", \"huh\", \"time\", \"nye\", \"na\", \"buh\", \"terrific\", \"heard\"]\n",
    "all_persons = [noun for noun in all_persons if noun not in not_persons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bb2ca51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Noun Mentions: 12706\n",
      "Total Person Mentions: 872\n",
      "Total Unique Noun Mentions: 3134\n",
      "Total Unique Person Mentions: 408\n"
     ]
    }
   ],
   "source": [
    "# Create a frequency distribution of nouns and persons\n",
    "noun_freq = FreqDist(all_nouns)\n",
    "person_freq = FreqDist(all_persons)\n",
    "\n",
    "# Total counts\n",
    "total_noun_mentions = sum(noun_freq.values())\n",
    "total_person_mentions = sum(person_freq.values())\n",
    "\n",
    "print(f'Total Noun Mentions: {total_noun_mentions}')\n",
    "print(f'Total Person Mentions: {total_person_mentions}')\n",
    "print(f'Total Unique Noun Mentions: {len(set(all_nouns))}')\n",
    "print(f'Total Unique Person Mentions: {len(set(all_persons))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f46bd",
   "metadata": {},
   "source": [
    "We have to consider that there are still errors in how the words were tagged, but from the obtained results Leonard mentioned 3134 unique nouns for a total of 12706 times and 408 unique person names for a total of 872 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f0a9832",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "cannot open resource",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Generate Wordclouds\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m noun_wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Sabandija-font-ffp.ttf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoun_freq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m person_wordcloud \u001b[38;5;241m=\u001b[39m WordCloud(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m, font_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Sabandija-font-ffp.ttf\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\u001b[38;5;241m.\u001b[39mgenerate_from_frequencies(person_freq)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Plot the Wordclouds\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\camd1\\miniconda3\\envs\\textmining2\\Lib\\site-packages\\wordcloud\\wordcloud.py:453\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    451\u001b[0m     font_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 453\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfrequencies\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmax_font_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;66;03m# find font sizes\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     sizes \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout_]\n",
      "File \u001b[1;32mc:\\Users\\camd1\\miniconda3\\envs\\textmining2\\Lib\\site-packages\\wordcloud\\wordcloud.py:503\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    500\u001b[0m tried_other_orientation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;66;03m# try to find a position\u001b[39;00m\n\u001b[1;32m--> 503\u001b[0m     font \u001b[38;5;241m=\u001b[39m \u001b[43mImageFont\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruetype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfont_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;66;03m# transpose font optionally\u001b[39;00m\n\u001b[0;32m    505\u001b[0m     transposed_font \u001b[38;5;241m=\u001b[39m ImageFont\u001b[38;5;241m.\u001b[39mTransposedFont(\n\u001b[0;32m    506\u001b[0m         font, orientation\u001b[38;5;241m=\u001b[39morientation)\n",
      "File \u001b[1;32mc:\\Users\\camd1\\miniconda3\\envs\\textmining2\\Lib\\site-packages\\PIL\\ImageFont.py:797\u001b[0m, in \u001b[0;36mtruetype\u001b[1;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FreeTypeFont(font, size, index, encoding, layout_engine)\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfreetype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_path(font):\n",
      "File \u001b[1;32mc:\\Users\\camd1\\miniconda3\\envs\\textmining2\\Lib\\site-packages\\PIL\\ImageFont.py:794\u001b[0m, in \u001b[0;36mtruetype.<locals>.freetype\u001b[1;34m(font)\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfreetype\u001b[39m(font):\n\u001b[1;32m--> 794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFreeTypeFont\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout_engine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\camd1\\miniconda3\\envs\\textmining2\\Lib\\site-packages\\PIL\\ImageFont.py:226\u001b[0m, in \u001b[0;36mFreeTypeFont.__init__\u001b[1;34m(self, font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    224\u001b[0m                 load_from_bytes(f)\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfont \u001b[38;5;241m=\u001b[39m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetfont\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout_engine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayout_engine\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m     load_from_bytes(font)\n",
      "\u001b[1;31mOSError\u001b[0m: cannot open resource"
     ]
    }
   ],
   "source": [
    "# Generate Wordclouds\n",
    "noun_wordcloud = WordCloud(width=800, height=400, background_color='white', font_path=\"./Sabandija-font-ffp.ttf\", random_state=12).generate_from_frequencies(noun_freq)\n",
    "person_wordcloud = WordCloud(width=800, height=400, background_color='white', font_path=\"./Sabandija-font-ffp.ttf\", random_state=12).generate_from_frequencies(person_freq)\n",
    "\n",
    "# Plot the Wordclouds\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(noun_wordcloud, interpolation='bilinear')\n",
    "plt.title('Noun Wordcloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(person_wordcloud, interpolation='bilinear')\n",
    "plt.title('Person Name Wordcloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07678838",
   "metadata": {},
   "source": [
    "First thing to notice is that there are some errors in both groups, mostly among the person names (eg. \"need\" is classified as a noun and \"vacuum\" is classified as a person name).  \n",
    "The mostly used nouns are 'time', 'guy', 'thing', 'night' and 'way'. The mostly used person names are 'sheldon', 'howard', 'bernadette', 'cooper' and 'leonard'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b63cb33-a920-4898-9b23-a33bbb41f3ec",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "What are the most important words mentioned by your character? Do this analysis per episode, per season and overall over the first 10 seasons. To achieve this task, please first make a bag-of-words and/or use the TF-IDF statistical principle. Remark: You can try to make a Wordcloud for visualization, based on the given bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f2bd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Combine tokens_no_stop into a single string for each row\n",
    "leonard_df['text_combined'] = leonard_df['tokens_no_stop'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Bag-of-Words representation\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_matrix = count_vectorizer.fit_transform(leonard_df['dialogue_no_stop'])\n",
    "\n",
    "# TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(leonard_df['dialogue_no_stop'])\n",
    "\n",
    "# Convert the matrices into DataFrames\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Combine the DataFrames with the original DataFrame\n",
    "leonard_df = pd.concat([leonard_df, bow_df, tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29995300-fc9c-4a3a-a372-c9cd93783c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by episode and season\n",
    "episode_bow = leonard_df.groupby('episode_name')[count_vectorizer.get_feature_names_out()].sum()\n",
    "episode_tfidf = leonard_df.groupby('episode_name')[tfidf_vectorizer.get_feature_names_out()].sum()\n",
    "\n",
    "season_bow = leonard_df.groupby('season_nr')[count_vectorizer.get_feature_names_out()].sum()\n",
    "season_tfidf = leonard_df.groupby('season_nr')[tfidf_vectorizer.get_feature_names_out()].sum()\n",
    "\n",
    "# Overall analysis\n",
    "overall_bow = bow_df.sum()\n",
    "overall_tfidf = tfidf_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "837d1765-5cac-40b4-ac30-86fc09d0bbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words per season (BoW):\n",
      "season_nr\n",
      "1.0        [sheldon, oh, know, yeah, penny]\n",
      "2.0           [oh, know, sheldon, hey, get]\n",
      "3.0     [000318914, 000318914, 10, 10, 100]\n",
      "4.0     [000318914, 000318914, 10, 10, 100]\n",
      "5.0     [000318914, 000318914, 10, 10, 100]\n",
      "6.0     [000318914, 000318914, 10, 10, 100]\n",
      "7.0     [000318914, 000318914, 10, 10, 100]\n",
      "8.0     [000318914, 000318914, 10, 10, 100]\n",
      "9.0     [000318914, 000318914, 10, 10, 100]\n",
      "10.0    [000318914, 000318914, 10, 10, 100]\n",
      "dtype: object\n",
      "\n",
      "Top 5 words per season (TF-IDF):\n",
      "season_nr\n",
      "1.0        [sheldon, oh, know, yeah, penny]\n",
      "2.0           [oh, know, sheldon, hey, get]\n",
      "3.0     [000318914, 000318914, 10, 10, 100]\n",
      "4.0     [000318914, 000318914, 10, 10, 100]\n",
      "5.0     [000318914, 000318914, 10, 10, 100]\n",
      "6.0     [000318914, 000318914, 10, 10, 100]\n",
      "7.0     [000318914, 000318914, 10, 10, 100]\n",
      "8.0     [000318914, 000318914, 10, 10, 100]\n",
      "9.0     [000318914, 000318914, 10, 10, 100]\n",
      "10.0    [000318914, 000318914, 10, 10, 100]\n",
      "dtype: object\n",
      "\n",
      "Top 5 words per episode (BoW):\n",
      "episode_name\n",
      "Series 01 Episode 01 – Pilot Episode                            [well, oh, go, good, penny]\n",
      "Series 01 Episode 02 – The Big Bran Hypothesis         [penny, sheldon, happy, penny, want]\n",
      "Series 01 Episode 03 – The Fuzzy Boots Corollary          [sheldon, howard, going, oh, get]\n",
      "Series 01 Episode 04 – The Luminous Fish Effect           [time, going, machine, okay, one]\n",
      "Series 01 Episode 05 – The Hamburger Postulate                    [sure, go, going, oh, uh]\n",
      "                                                                       ...                 \n",
      "Series 10 Episode 20 – The Recollection Dissipation     [000318914, 000318914, 10, 10, 100]\n",
      "Series 10 Episode 21 – The Separation Agitation         [000318914, 000318914, 10, 10, 100]\n",
      "Series 10 Episode 22 – The Cognition Regeneration       [000318914, 000318914, 10, 10, 100]\n",
      "Series 10 Episode 23 – The Gyroscopic Collapse          [000318914, 000318914, 10, 10, 100]\n",
      "Series 10 Episode 24 – The Long Distance Dissonance     [000318914, 000318914, 10, 10, 100]\n",
      "Length: 231, dtype: object\n",
      "\n",
      "Top 5 words per episode (TF-IDF):\n",
      "episode_name\n",
      "Series 01 Episode 01 – Pilot Episode                            [well, oh, go, good, penny]\n",
      "Series 01 Episode 02 – The Big Bran Hypothesis         [penny, sheldon, happy, penny, want]\n",
      "Series 01 Episode 03 – The Fuzzy Boots Corollary          [sheldon, howard, going, oh, get]\n",
      "Series 01 Episode 04 – The Luminous Fish Effect           [time, going, machine, okay, one]\n",
      "Series 01 Episode 05 – The Hamburger Postulate                    [sure, go, going, oh, uh]\n",
      "                                                                       ...                 \n",
      "Series 10 Episode 20 – The Recollection Dissipation     [000318914, 000318914, 10, 10, 100]\n",
      "Series 10 Episode 21 – The Separation Agitation         [000318914, 000318914, 10, 10, 100]\n",
      "Series 10 Episode 22 – The Cognition Regeneration       [000318914, 000318914, 10, 10, 100]\n",
      "Series 10 Episode 23 – The Gyroscopic Collapse          [000318914, 000318914, 10, 10, 100]\n",
      "Series 10 Episode 24 – The Long Distance Dissonance     [000318914, 000318914, 10, 10, 100]\n",
      "Length: 231, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def extract_top_words_per_row(matrix, top_n=5):\n",
    "    # Get the top N words for each row\n",
    "    top_words_per_row = matrix.apply(lambda row: row.nlargest(top_n).index.tolist(), axis=1)\n",
    "    return top_words_per_row\n",
    "\n",
    "# Extract top words for each row\n",
    "top_words_per_row_season_bow = extract_top_words_per_row(season_bow)\n",
    "top_words_per_row_season_tfidf = extract_top_words_per_row(season_tfidf)\n",
    "top_words_per_row_episode_bow = extract_top_words_per_row(episode_bow)\n",
    "top_words_per_row_episode_tfidf = extract_top_words_per_row(episode_tfidf)\n",
    "\n",
    "# Display the top words for each row\n",
    "print(\"Top 5 words per season (BoW):\")\n",
    "print(top_words_per_row_season_bow)\n",
    "\n",
    "print(\"\\nTop 5 words per season (TF-IDF):\")\n",
    "print(top_words_per_row_season_tfidf)\n",
    "\n",
    "print(\"\\nTop 5 words per episode (BoW):\")\n",
    "print(top_words_per_row_episode_bow)\n",
    "\n",
    "print(\"\\nTop 5 words per episode (TF-IDF):\")\n",
    "print(top_words_per_row_episode_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de289f6-fd4a-4f27-a4f9-44eb1c527491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
