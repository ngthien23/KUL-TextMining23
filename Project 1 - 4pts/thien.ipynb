{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camd1\\miniconda3\\envs\\textmining2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Scripts TBBT.csv\", sep = \",\", skipinitialspace=True, engine=\"python\")\n",
    "leonard_df = df[df['person_scene'] == 'Leonard'].copy()\n",
    "\n",
    "# Extracting season and episode numbers using regular expressions\n",
    "leonard_df[['season', 'episode']] = leonard_df['episode_name'].str.extract(r'Series (\\d+) Episode (\\d+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - done\n",
    "On average, how many sentences and words does your character have to speak per\n",
    "episode? Does this deviate across seasons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sent_tokenize() also counts the sentences inside parenthesis, which is technically not a sentence spoken by the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentences in each dialogue (row)\n",
    "leonard_df['num_sentences'] = leonard_df['dialogue'].apply(lambda x: len(sent_tokenize(str(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change this section: tokenize before punctuation remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and lowercase function\n",
    "def remove_punctuation(text):\n",
    "    # Replace specific punctuation marks with spaces\n",
    "    text = text.replace(\"\\u2019\", ' ') # Unicode for apostrophe\n",
    "    text = text.replace(\"\\u2013\", ' ')  # Unicode for en dash\n",
    "    text = text.replace(\"\\u2014\", ' ')  # Unicode for em dash\n",
    "    text = text.replace(\"-\", ' ')\n",
    "    text = text.replace(\"\\u2026\", ' ')  # Unicode for ellipsis\n",
    "    # Remove remaining punctuation marks\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    return text.lower()\n",
    "\n",
    "# Remove punctuation and lowercase dialogues\n",
    "leonard_df['dialogue_no_punct'] = leonard_df['dialogue'].apply(remove_punctuation)\n",
    "\n",
    "# Number of words per dialogue (row)\n",
    "leonard_df['num_words'] = leonard_df['dialogue_no_punct'].apply(lambda x: len(word_tokenize(str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences per episode\n",
    "sentences_per_episode = leonard_df.groupby('episode_name')['num_sentences'].sum().reset_index()\n",
    "\n",
    "# Average sentences per episode\n",
    "avg_sentences_per_episode = sentences_per_episode['num_sentences'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words per episode\n",
    "words_per_episode = leonard_df.groupby('episode_name')['num_words'].sum().reset_index()\n",
    "\n",
    "# Average words per episode\n",
    "avg_words_per_episode = words_per_episode['num_words'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leonard_df['season_nr'] = leonard_df['episode_name'].str.extract(r'Series (\\d+)')\n",
    "leonard_df['season_nr'] = leonard_df['season_nr'].astype(float)\n",
    "\n",
    "sentences_per_season_episode = leonard_df.groupby(['season_nr', 'episode_name'])['num_sentences'].sum().reset_index()\n",
    "words_per_season_episode = leonard_df.groupby(['season_nr', 'episode_name'])['num_words'].sum().reset_index()\n",
    "\n",
    "# Average sentences per season-episode\n",
    "avg_sentences_per_season = sentences_per_season_episode.groupby('season_nr')['num_sentences'].mean().reset_index()\n",
    "\n",
    "# Average words per season-episode\n",
    "avg_words_per_season = words_per_season_episode.groupby('season_nr')['num_words'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Leonard's sentences per episode:\", round(avg_sentences_per_episode))\n",
    "print(\"Average Leonard's sentences per season-episode:\", round(avg_sentences_per_season))\n",
    "print(\"Average Leonard's words per episode:\", round(avg_words_per_episode))\n",
    "print(\"Average Leonard's words per season-episode:\", round(avg_words_per_season))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Globally, over all episodes within the first 10 seasons, how many times does your character mention nouns, and person names? Make a Wordcloud of this tag/entity to have a clear visualization which nouns/person names are mostly used by your character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def extract_persons(tokens):\n",
    "    persons = []\n",
    "    ner_persons = nlp(tokens)\n",
    "    for result in ner_persons:\n",
    "        if result['entity'] == 'B-PER':\n",
    "            persons.append(result['word'])\n",
    "    return persons\n",
    "\n",
    "leonard_df['persons'] = leonard_df['dialogue'].apply(extract_persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54080                                []\n",
       "49809                                []\n",
       "41981    [J, ##ab, ##ba, J, ##ab, ##ba]\n",
       "13113             [Benedict, Ju, ##das]\n",
       "33721                                []\n",
       "Name: persons, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leonard_df['persons'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird result for person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"QCRI/bert-base-multilingual-cased-pos-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"QCRI/bert-base-multilingual-cased-pos-english\")\n",
    "nlp = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "def pos_tag(tokens):\n",
    "    entities = {'nouns': [], 'persons': []}\n",
    "    results = nlp(tokens)\n",
    "    for result in results:\n",
    "        if result['entity'] == 'NN':\n",
    "            entities['nouns'].append(result['word'])\n",
    "        elif result['entity'] == 'NNP':\n",
    "            entities['persons'].append(result['word'])\n",
    "    return entities\n",
    "\n",
    "leonard_df['entities'] = leonard_df['dialogue'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42667    []\n",
       "32749    []\n",
       "36145    []\n",
       "3659     []\n",
       "20975    []\n",
       "Name: persons, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leonard_df['entities'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency distribution of nouns and persons\n",
    "noun_freq = FreqDist(leonard_df['nouns'])\n",
    "person_freq = FreqDist(leonard_df['persons'])\n",
    "\n",
    "# Total counts\n",
    "total_noun_mentions = sum(noun_freq.values())\n",
    "total_person_mentions = sum(person_freq.values())\n",
    "\n",
    "print(f'Total Noun Mentions: {total_noun_mentions}')\n",
    "print(f'Total Person Mentions: {total_person_mentions}')\n",
    "print(f'Total Unique Noun Mentions: {len(set(nouns))}')\n",
    "print(f'Total Unique Person Mentions: {len(set(persons))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "What are the most important words mentioned by your character? Do this analysis per episode, per season and overall over the first 10 seasons. To achieve this task, please first make a bag-of-words and/or use the TF-IDF statistical principle. Remark: You can try to make a Wordcloud for visualization, based on the given bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '…' and '-'\n",
    "leonard_df['dialogue'] = leonard_df['dialogue'].str.replace('…', '')\n",
    "leonard_df['dialogue'] = leonard_df['dialogue'].str.replace('-', '')\n",
    "# Tokenize and lowercase dialogues\n",
    "leonard_df['tokenized'] = leonard_df['dialogue'].apply(lambda x: word_tokenize(str(x).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation from tokenized data\n",
    "def remove_punctuation(tokens):\n",
    "    return [token for token in tokens if token not in string.punctuation and token != '’']\n",
    "\n",
    "# Removing punctuation from tokenized data\n",
    "leonard_df['no_punctuation'] = leonard_df['tokenized'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sorted bag of words\n",
    "def create_bow(corpus):    \n",
    "    bow = set()\n",
    "    for doc in corpus:\n",
    "        bow = bow.union(set(doc))\n",
    "    return sorted(bow)\n",
    "\n",
    "# Find 10 most important words using TF-IDF\n",
    "def imp_tf_idf(corpus, bow):\n",
    "    # Calculate term frequency (TF)\n",
    "    n_docs = len(corpus)         # Number of documents in the corpus\n",
    "    n_words_set = len(bow)       # Number of unique words in the corpus\n",
    "\n",
    "    # Initialize TF matrix\n",
    "    tf_matrix = np.zeros((n_docs, n_words_set))\n",
    "\n",
    "    for i in range(n_docs):\n",
    "        n_words = len(corpus[i])    # Number of words in the document\n",
    "        for w in corpus[i]:\n",
    "            word_index = bow.index(w)\n",
    "            tf_matrix[i][word_index] += 1 / n_words\n",
    "        \n",
    "    # Calculate Inverse Document Frequency (IDF)\n",
    "    idf = np.zeros(n_words_set)\n",
    "    for idx, w in enumerate(bow):\n",
    "        k = sum(1 for doc in corpus if w in doc)\n",
    "        idf[idx] = np.log10(n_docs / k) if k != 0 else 0\n",
    "\n",
    "    # Calculate TF-IDF matrix\n",
    "    tf_idf_matrix = tf_matrix * idf\n",
    "\n",
    "    # Find most important words in the corpus\n",
    "    flat_tf_idf = tf_idf_matrix.flatten()\n",
    "    sorted_indices = np.argsort(flat_tf_idf)\n",
    "    highest_indices = sorted_indices[-10:][::-1]\n",
    "    highest_values = [bow[i // n_docs] for i in highest_indices]\n",
    "    return highest_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bag-of-word and apply TF-IDF to determine most important word\n",
    "1) per episode:\n",
    "    documents = dialogues\n",
    "    corpus = episode\n",
    "2) per season:\n",
    "    documents = dialogues\n",
    "    corpus = season\n",
    "3) overall over 10 seasons\n",
    "    documents = dialogues\n",
    "    corpus = whole 10 seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    season episode                                              words\n",
      "0       01      01  [so, rest, don, good, behaves, based, colon, d...\n",
      "1       01      02  [only, but, things, loud, leaves, talk, lets, ...\n",
      "2       01      03  [you, prove, good, sub, right, quite, didn, wo...\n",
      "3       01      04  [shake, give, apartment, day, slices, look, lo...\n",
      "4       01      05  [weren, grow, or, research, hey, no, this, be,...\n",
      "..     ...     ...                                                ...\n",
      "226     10      20  [is, to, be, understood, be, pretend, plan, wh...\n",
      "227     10      21  [of, go, have, t, that, until, any, might, reb...\n",
      "228     10      22  [t, out, hi, and, doesn, still, thing, child, ...\n",
      "229     10      23  [opportunity, doing, could, on, on, all, with,...\n",
      "230     10      24  [follow, pool, ask, at, yeah, yeah, right, how...\n",
      "\n",
      "[231 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Per episode: documents = dialogues; corpus = episode\n",
    "result1 = []\n",
    "for season in leonard_df['season'].unique():\n",
    "    for episode in leonard_df[leonard_df['season'] == season]['episode'].unique():\n",
    "        corpus = []\n",
    "        for row in leonard_df[(leonard_df['season'] == season) & (leonard_df['episode'] == episode)].index:\n",
    "            corpus.append(leonard_df['no_punctuation'][row])\n",
    "\n",
    "        # Add words to result\n",
    "        important_words = imp_tf_idf(corpus, create_bow(corpus))\n",
    "        result1.append({'season': season, 'episode': episode, 'words': important_words})\n",
    "\n",
    "result1 = pd.DataFrame(result1)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  season                                              words\n",
      "0     01  [drink, off, stands, push, spare, cover, drive...\n",
      "1     02  [racist, eye, come, dirty, miracle, sit, throu...\n",
      "2     03  [discarded, standard, at, airplane, announce, ...\n",
      "3     04  [quiz, fit, today, stuck, buddies, trying, yam...\n",
      "4     05  [sweaty, leaving, rash, saturday, that, game, ...\n",
      "5     06  [surprises, chided, no, dog, okay, saying, bec...\n",
      "6     07  [how, seriously, inappropriate, audition, hone...\n",
      "7     08  [situation, artistic, ladies, moth, grounds, b...\n",
      "8     09  [ha, counsellor, try, calm, you, foreever, pot...\n",
      "9     10  [tried, brought, snapchat, things, sorry, met,...\n"
     ]
    }
   ],
   "source": [
    "# per season: documents = dialogues; corpus = season\n",
    "result2 = []\n",
    "for season in leonard_df['season'].unique():\n",
    "    corpus = []\n",
    "    for row in leonard_df[(leonard_df['season'] == season)].index:\n",
    "        corpus.append(leonard_df['no_punctuation'][row])\n",
    "        \n",
    "    # Add words to result\n",
    "    result2.append({'season': season, 'words': imp_tf_idf(corpus, create_bow(corpus))})\n",
    "\n",
    "result2 = pd.DataFrame(result2)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          words\n",
      "0      curtains\n",
      "1          exam\n",
      "2       haircut\n",
      "3   introducing\n",
      "4    insightful\n",
      "5  dissertation\n",
      "6         birth\n",
      "7         world\n",
      "8      nibbling\n",
      "9     confident\n"
     ]
    }
   ],
   "source": [
    "# whole: documents = dialogues; corpus = whole\n",
    "corpus = []\n",
    "for row in leonard_df.index:\n",
    "    corpus.append(leonard_df['no_punctuation'][row])\n",
    "\n",
    "# Add words to result\n",
    "result3 = imp_tf_idf(corpus, create_bow(corpus))\n",
    "\n",
    "result3 = pd.DataFrame(result3, columns=['words'])\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Examine the co-occurence of words for your character by using the Positive Pointwise Mutual Information measurement. Which words are commonly used together in his/her dialogues? Remark: You can try to make a Word-Word co-occurence matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 word pairs with Positive Pointwise Mutual Information (PMI):\n",
      "Papa-Doc: PMI = 17.2665\n",
      "fractional-T1: PMI = 17.2665\n",
      "T1-bandwidth: PMI = 17.2665\n",
      "heterosexual-bedrooms: PMI = 17.2665\n",
      "However-briefly: PMI = 17.2665\n",
      "biological-impossibility: PMI = 17.2665\n",
      "fibre-content: PMI = 17.2665\n",
      "atomic-bomb: PMI = 17.2665\n",
      "overflow-reservoir: PMI = 17.2665\n",
      "crossbar-H: PMI = 17.2665\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from math import log2\n",
    "\n",
    "dialogues = leonard_df['dialogue'].tolist()\n",
    "\n",
    "# Step 1: Tokenize by sentence\n",
    "tokenized_sentences = [sent_tokenize(dialogue) for dialogue in dialogues]\n",
    "\n",
    "# Step 2: Tokenize by word, add <S> and <E>\n",
    "tokenized_words = []\n",
    "for sentences in tokenized_sentences:\n",
    "    dialogue_tokens = []\n",
    "    for sentence in sentences:\n",
    "        words = ['<S>'] + word_tokenize(sentence) + ['<E>']\n",
    "        dialogue_tokens.extend(words)\n",
    "    tokenized_words.append(dialogue_tokens)\n",
    "\n",
    "# Step 3: Create Word-Word co-occurrence matrix\n",
    "word_count = defaultdict(int)\n",
    "co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for dialogue_tokens in tokenized_words:\n",
    "    for i in range(len(dialogue_tokens)):\n",
    "        word_count[dialogue_tokens[i]] += 1\n",
    "        if i < len(dialogue_tokens) - 1:\n",
    "            co_occurrence_matrix[dialogue_tokens[i]][dialogue_tokens[i + 1]] += 1\n",
    "\n",
    "# Step 4: Calculate Pointwise Mutual Information (PMI) for word pairs\n",
    "total_words = sum(word_count.values())\n",
    "pmi_scores = defaultdict(float)\n",
    "\n",
    "for word1, word2_counts in co_occurrence_matrix.items():\n",
    "    for word2, count in word2_counts.items():\n",
    "        co_occurrence = count\n",
    "        word1_count = word_count[word1]\n",
    "        word2_count = word_count[word2]\n",
    "        \n",
    "        pmi = log2((co_occurrence / total_words) / ((word1_count / total_words) * (word2_count / total_words)))\n",
    "        if pmi > 0:\n",
    "            word_pair = f\"{word1}-{word2}\"\n",
    "            pmi_scores[word_pair] = pmi\n",
    "\n",
    "# Get top 10 word pairs based on PMI\n",
    "top_10_pmi = sorted(pmi_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Top 10 word pairs with Positive Pointwise Mutual Information (PMI):\")\n",
    "for pair, pmi_score in top_10_pmi:\n",
    "    print(f\"{pair}: PMI = {pmi_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
