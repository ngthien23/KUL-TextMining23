{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camd1\\miniconda3\\envs\\textmining2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Scripts TBBT.csv\", sep = \",\", skipinitialspace=True, engine=\"python\")\n",
    "leonard_df = df[df['person_scene'] == 'Leonard'].copy()\n",
    "\n",
    "# Extracting season and episode numbers using regular expressions\n",
    "leonard_df[['season', 'episode']] = leonard_df['episode_name'].str.extract(r'Series (\\d+) Episode (\\d+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - done\n",
    "On average, how many sentences and words does your character have to speak per\n",
    "episode? Does this deviate across seasons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sent_tokenize() also counts the sentences inside parenthesis, which is technically not a sentence spoken by the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentences in each dialogue (row)\n",
    "leonard_df['num_sentences'] = leonard_df['dialogue'].apply(lambda x: len(sent_tokenize(str(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change this section: tokenize before punctuation remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation and lowercase function\n",
    "def remove_punctuation(text):\n",
    "    # Replace specific punctuation marks with spaces\n",
    "    text = text.replace(\"\\u2019\", ' ') # Unicode for apostrophe\n",
    "    text = text.replace(\"\\u2013\", ' ')  # Unicode for en dash\n",
    "    text = text.replace(\"\\u2014\", ' ')  # Unicode for em dash\n",
    "    text = text.replace(\"-\", ' ')\n",
    "    text = text.replace(\"\\u2026\", ' ')  # Unicode for ellipsis\n",
    "    # Remove remaining punctuation marks\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    return text.lower()\n",
    "\n",
    "# Remove punctuation and lowercase dialogues\n",
    "leonard_df['dialogue_no_punct'] = leonard_df['dialogue'].apply(remove_punctuation)\n",
    "\n",
    "# Number of words per dialogue (row)\n",
    "leonard_df['num_words'] = leonard_df['dialogue_no_punct'].apply(lambda x: len(word_tokenize(str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences per episode\n",
    "sentences_per_episode = leonard_df.groupby('episode_name')['num_sentences'].sum().reset_index()\n",
    "\n",
    "# Average sentences per episode\n",
    "avg_sentences_per_episode = sentences_per_episode['num_sentences'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words per episode\n",
    "words_per_episode = leonard_df.groupby('episode_name')['num_words'].sum().reset_index()\n",
    "\n",
    "# Average words per episode\n",
    "avg_words_per_episode = words_per_episode['num_words'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leonard_df['season_nr'] = leonard_df['episode_name'].str.extract(r'Series (\\d+)')\n",
    "leonard_df['season_nr'] = leonard_df['season_nr'].astype(float)\n",
    "\n",
    "sentences_per_season_episode = leonard_df.groupby(['season_nr', 'episode_name'])['num_sentences'].sum().reset_index()\n",
    "words_per_season_episode = leonard_df.groupby(['season_nr', 'episode_name'])['num_words'].sum().reset_index()\n",
    "\n",
    "# Average sentences per season-episode\n",
    "avg_sentences_per_season = sentences_per_season_episode.groupby('season_nr')['num_sentences'].mean().reset_index()\n",
    "\n",
    "# Average words per season-episode\n",
    "avg_words_per_season = words_per_season_episode.groupby('season_nr')['num_words'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Leonard's sentences per episode:\", round(avg_sentences_per_episode))\n",
    "print(\"Average Leonard's sentences per season-episode:\", round(avg_sentences_per_season))\n",
    "print(\"Average Leonard's words per episode:\", round(avg_words_per_episode))\n",
    "print(\"Average Leonard's words per season-episode:\", round(avg_words_per_season))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Globally, over all episodes within the first 10 seasons, how many times does your character mention nouns, and person names? Make a Wordcloud of this tag/entity to have a clear visualization which nouns/person names are mostly used by your character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def extract_persons(tokens):\n",
    "    persons = []\n",
    "    ner_persons = nlp(tokens)\n",
    "    for result in ner_persons:\n",
    "        if result['entity'] == 'B-PER':\n",
    "            persons.append(result['word'])\n",
    "    return persons\n",
    "\n",
    "leonard_df['persons'] = leonard_df['dialogue'].apply(extract_persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"QCRI/bert-base-multilingual-cased-pos-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"QCRI/bert-base-multilingual-cased-pos-english\")\n",
    "nlp = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "def extract_nouns(tokens):\n",
    "    nouns = []\n",
    "    pos_nouns = nlp(tokens)\n",
    "    for result in pos_nouns:\n",
    "        if result['entity'] == 'NN':\n",
    "            nouns.append(result['word'])\n",
    "    return nouns\n",
    "\n",
    "leonard_df['nouns'] = leonard_df['dialogue'].apply(extract_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leonard_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency distribution of nouns and persons\n",
    "noun_freq = FreqDist(leonard_df['nouns'])\n",
    "person_freq = FreqDist(leonard_df['persons'])\n",
    "\n",
    "# Total counts\n",
    "total_noun_mentions = sum(noun_freq.values())\n",
    "total_person_mentions = sum(person_freq.values())\n",
    "\n",
    "print(f'Total Noun Mentions: {total_noun_mentions}')\n",
    "print(f'Total Person Mentions: {total_person_mentions}')\n",
    "print(f'Total Unique Noun Mentions: {len(set(nouns))}')\n",
    "print(f'Total Unique Person Mentions: {len(set(persons))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "What are the most important words mentioned by your character? Do this analysis per episode, per season and overall over the first 10 seasons. To achieve this task, please first make a bag-of-words and/or use the TF-IDF statistical principle. Remark: You can try to make a Wordcloud for visualization, based on the given bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '…' and '-'\n",
    "leonard_df['dialogue'] = leonard_df['dialogue'].str.replace('…', '')\n",
    "leonard_df['dialogue'] = leonard_df['dialogue'].str.replace('-', '')\n",
    "# Tokenize and lowercase dialogues\n",
    "leonard_df['tokenized'] = leonard_df['dialogue'].apply(lambda x: word_tokenize(str(x).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation from tokenized data\n",
    "def remove_punctuation(tokens):\n",
    "    return [token for token in tokens if token not in string.punctuation and token != '’']\n",
    "\n",
    "# Removing punctuation from tokenized data\n",
    "leonard_df['no_punctuation'] = leonard_df['tokenized'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create sorted bag of words\n",
    "def create_bow(corpus):    \n",
    "    bow = set()\n",
    "    for doc in corpus:\n",
    "        bow = bow.union(set(doc))\n",
    "    return sorted(bow)\n",
    "\n",
    "# Find 10 most important words using TF-IDF\n",
    "def imp_tf_idf(corpus, bow):\n",
    "    # Calculate term frequency (TF)\n",
    "    n_docs = len(corpus)         # Number of documents in the corpus\n",
    "    n_words_set = len(bow)       # Number of unique words in the corpus\n",
    "\n",
    "    # Initialize TF matrix\n",
    "    tf_matrix = np.zeros((n_docs, n_words_set))\n",
    "\n",
    "    for i in range(n_docs):\n",
    "        n_words = len(corpus[i])    # Number of words in the document\n",
    "        for w in corpus[i]:\n",
    "            word_index = bow.index(w)\n",
    "            tf_matrix[i][word_index] += 1 / n_words\n",
    "        \n",
    "    # Calculate Inverse Document Frequency (IDF)\n",
    "    idf = np.zeros(n_words_set)\n",
    "    for idx, w in enumerate(bow):\n",
    "        k = sum(1 for doc in corpus if w in doc)\n",
    "        idf[idx] = np.log10(n_docs / k) if k != 0 else 0\n",
    "\n",
    "    # Calculate TF-IDF matrix\n",
    "    tf_idf_matrix = tf_matrix * idf\n",
    "\n",
    "    # Find most important words in the corpus\n",
    "    flat_tf_idf = tf_idf_matrix.flatten()\n",
    "    sorted_indices = np.argsort(flat_tf_idf)\n",
    "    highest_indices = sorted_indices[-10:][::-1]\n",
    "    highest_values = [bow[i // n_docs] for i in highest_indices]\n",
    "    return highest_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bag-of-word and apply TF-IDF to determine most important word\n",
    "1) per episode:\n",
    "    documents = dialogues\n",
    "    corpus = episode\n",
    "2) per season:\n",
    "    documents = dialogues\n",
    "    corpus = season\n",
    "3) overall over 10 seasons\n",
    "    documents = dialogues\n",
    "    corpus = whole 10 seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    season episode                                              words\n",
      "0       01      01  [so, rest, don, good, behaves, based, colon, d...\n",
      "1       01      02  [only, but, things, loud, leaves, talk, lets, ...\n",
      "2       01      03  [you, prove, good, sub, right, quite, didn, wo...\n",
      "3       01      04  [shake, give, apartment, day, slices, look, lo...\n",
      "4       01      05  [weren, grow, or, research, hey, no, this, be,...\n",
      "..     ...     ...                                                ...\n",
      "226     10      20  [is, to, be, understood, be, pretend, plan, wh...\n",
      "227     10      21  [of, go, have, t, that, until, any, might, reb...\n",
      "228     10      22  [t, out, hi, and, doesn, still, thing, child, ...\n",
      "229     10      23  [opportunity, doing, could, on, on, all, with,...\n",
      "230     10      24  [follow, pool, ask, at, yeah, yeah, right, how...\n",
      "\n",
      "[231 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Per episode: documents = dialogues; corpus = episode\n",
    "result1 = []\n",
    "for season in leonard_df['season'].unique():\n",
    "    for episode in leonard_df[leonard_df['season'] == season]['episode'].unique():\n",
    "        corpus = []\n",
    "        for row in leonard_df[(leonard_df['season'] == season) & (leonard_df['episode'] == episode)].index:\n",
    "            corpus.append(leonard_df['no_punctuation'][row])\n",
    "\n",
    "        # Add words to result\n",
    "        important_words = imp_tf_idf(corpus, create_bow(corpus))\n",
    "        result1.append({'season': season, 'episode': episode, 'words': important_words})\n",
    "\n",
    "result1 = pd.DataFrame(result1)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  season                                              words\n",
      "0     01  [drink, off, stands, push, spare, cover, drive...\n",
      "1     02  [racist, eye, come, dirty, miracle, sit, throu...\n",
      "2     03  [discarded, standard, at, airplane, announce, ...\n",
      "3     04  [quiz, fit, today, stuck, buddies, trying, yam...\n",
      "4     05  [sweaty, leaving, rash, saturday, that, game, ...\n",
      "5     06  [surprises, chided, no, dog, okay, saying, bec...\n",
      "6     07  [how, seriously, inappropriate, audition, hone...\n",
      "7     08  [situation, artistic, ladies, moth, grounds, b...\n",
      "8     09  [ha, counsellor, try, calm, you, foreever, pot...\n",
      "9     10  [tried, brought, snapchat, things, sorry, met,...\n"
     ]
    }
   ],
   "source": [
    "# per season: documents = dialogues; corpus = season\n",
    "result2 = []\n",
    "for season in leonard_df['season'].unique():\n",
    "    corpus = []\n",
    "    for row in leonard_df[(leonard_df['season'] == season)].index:\n",
    "        corpus.append(leonard_df['no_punctuation'][row])\n",
    "        \n",
    "    # Add words to result\n",
    "    result2.append({'season': season, 'words': imp_tf_idf(corpus, create_bow(corpus))})\n",
    "\n",
    "result2 = pd.DataFrame(result2)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          words\n",
      "0      curtains\n",
      "1          exam\n",
      "2       haircut\n",
      "3   introducing\n",
      "4    insightful\n",
      "5  dissertation\n",
      "6         birth\n",
      "7         world\n",
      "8      nibbling\n",
      "9     confident\n"
     ]
    }
   ],
   "source": [
    "# whole: documents = dialogues; corpus = whole\n",
    "corpus = []\n",
    "for row in leonard_df.index:\n",
    "    corpus.append(leonard_df['no_punctuation'][row])\n",
    "\n",
    "# Add words to result\n",
    "result3 = imp_tf_idf(corpus, create_bow(corpus))\n",
    "\n",
    "result3 = pd.DataFrame(result3, columns=['words'])\n",
    "print(result3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
