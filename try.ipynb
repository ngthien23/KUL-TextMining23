{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camd1\\miniconda3\\envs\\textmining2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\camd1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
      "Document 2: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
      "Document 3: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]\n",
      "Document 4: [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Document 5: [1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"My name is Thien.\",\n",
    "    \"My friend's name is Marcus.\",\n",
    "    \"We live together.\",\n",
    "    \"Miwa sometimes visits us.\",\n",
    "    \"Do you want to visit us?\",\n",
    "]\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokens = [doc.split() for doc in documents]\n",
    "\n",
    "# Step 2: Building Vocabulary\n",
    "vocabulary = set(word for doc in tokens for word in doc)\n",
    "\n",
    "# Step 3: Vectorization\n",
    "def get_bow_vector(doc, vocab):\n",
    "    word_counts = Counter(doc)\n",
    "    bow_vector = [word_counts[word] if word in word_counts else 0 for word in vocab]\n",
    "    return bow_vector\n",
    "\n",
    "# Create Bag-of-Words vectors for each document\n",
    "bow_vectors = [get_bow_vector(doc, vocabulary) for doc in tokens]\n",
    "\n",
    "# Display the Bag-of-Words vectors\n",
    "for i, vec in enumerate(bow_vectors):\n",
    "    print(f\"Document {i+1}:\", vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   dialogue              nouns  num_nouns\n",
      "0  I would like to run a car over a turtle.  [car, tur, ##tle]          3\n",
      "1                                 Oh my god              [god]          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "pipeline = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "def extract_nouns(tokens):\n",
    "    nouns = []\n",
    "    pos_nouns = pipeline(tokens)\n",
    "    for result in pos_nouns:\n",
    "        if result['entity'] == 'NN':\n",
    "            nouns.append(result['word'])\n",
    "    return nouns\n",
    "\n",
    "# Create an example DataFrame\n",
    "example_df = pd.DataFrame({'dialogue': ['I would like to run a car over a turtle.', 'Oh my god']})\n",
    "\n",
    "# Apply the extract_nouns function to create the 'nouns' column\n",
    "example_df['nouns'] = example_df['dialogue'].apply(extract_nouns)\n",
    "example_df['num_nouns'] = example_df['dialogue'].apply(lambda x: len(extract_nouns(x)))\n",
    "\n",
    "# Print the example DataFrame\n",
    "print(example_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'PRP', 'score': 0.99967754, 'index': 1, 'word': 'I', 'start': 0, 'end': 1}, {'entity': 'MD', 'score': 0.999617, 'index': 2, 'word': 'would', 'start': 2, 'end': 7}, {'entity': 'VB', 'score': 0.99917704, 'index': 3, 'word': 'like', 'start': 8, 'end': 12}, {'entity': 'TO', 'score': 0.9998995, 'index': 4, 'word': 'to', 'start': 13, 'end': 15}, {'entity': 'VB', 'score': 0.99969435, 'index': 5, 'word': 'run', 'start': 16, 'end': 19}, {'entity': 'DT', 'score': 0.9998702, 'index': 6, 'word': 'a', 'start': 20, 'end': 21}, {'entity': 'NN', 'score': 0.9997905, 'index': 7, 'word': 'car', 'start': 22, 'end': 25}, {'entity': 'IN', 'score': 0.9996146, 'index': 8, 'word': 'over', 'start': 26, 'end': 30}, {'entity': 'DT', 'score': 0.99986935, 'index': 9, 'word': 'a', 'start': 31, 'end': 32}, {'entity': 'NN', 'score': 0.9996551, 'index': 10, 'word': 'tur', 'start': 33, 'end': 36}, {'entity': 'NN', 'score': 0.99923944, 'index': 11, 'word': '##tle', 'start': 36, 'end': 39}, {'entity': '.', 'score': 0.9999263, 'index': 12, 'word': '.', 'start': 39, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "outputs = pipeline(\"I would like to run a car over a turtle.\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': {'C', 'B', 'A'}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'A': [25, 30, 15, 40],\n",
    "    'B': [35, 20, 10, 50],\n",
    "    'C': [45, 10, 30, 60]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Find the 10 highest values and get their column names\n",
    "highest_values = df.stack().nlargest(10).index.get_level_values(1)\n",
    "\n",
    "# Add the column names of the 10 highest values to a set in result['words']\n",
    "result = {'words': set(highest_values)}\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sorted bag of words\n",
    "def create_bow(corpus):    \n",
    "    bow = set()\n",
    "    for doc in corpus:\n",
    "        bow = bow.union(set(doc))\n",
    "    return sorted(bow)\n",
    "\n",
    "# Find 10 most important words using TF-IDF\n",
    "def imp_tf_idf(corpus, bow):\n",
    "    # Calculate term frequency (TF)\n",
    "    n_docs = len(corpus)         # Number of documents in the corpus\n",
    "    n_words_set = len(bow)       # Number of unique words in the corpus\n",
    "\n",
    "    df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=bow)\n",
    "\n",
    "    for i in range(n_docs):\n",
    "        n_words = len(corpus[i])    # Number of words in the document\n",
    "        for w in corpus[i]:\n",
    "            df_tf[w][i] = df_tf[w][i] + (1 / n_words)\n",
    "        \n",
    "    # Calculate Inverse Document Frequency (IDF)\n",
    "    idf = {}\n",
    "    for w in bow:\n",
    "        k = 0    # number of documents in the corpus that contain this word\n",
    "\n",
    "        for i in range(n_docs):\n",
    "            if w in corpus[i]:\n",
    "                k += 1\n",
    "                \n",
    "        idf[w] =  np.log10(n_docs / k)\n",
    "\n",
    "    # Find most important words in the corpus\n",
    "    df_tf_idf = df_tf.copy()\n",
    "\n",
    "    for w in bow:\n",
    "        for i in range(n_docs):\n",
    "            df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "    \n",
    "    highest_values = df_tf_idf.stack().nlargest(10).index.get_level_values(1).tolist()\n",
    "    return highest_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create sorted bag of words\n",
    "def create_bow(corpus):    \n",
    "    bow = set()\n",
    "    for doc in corpus:\n",
    "        bow = bow.union(set(doc))\n",
    "    return sorted(bow)\n",
    "\n",
    "# Find 10 most important words using TF-IDF\n",
    "def imp_tf_idf(corpus, bow):\n",
    "    # Calculate term frequency (TF)\n",
    "    n_docs = len(corpus)         # Number of documents in the corpus\n",
    "    n_words_set = len(bow)       # Number of unique words in the corpus\n",
    "\n",
    "    # Initialize TF matrix\n",
    "    tf_matrix = np.zeros((n_docs, n_words_set))\n",
    "\n",
    "    for i in range(n_docs):\n",
    "        n_words = len(corpus[i])    # Number of words in the document\n",
    "        for w in corpus[i]:\n",
    "            word_index = bow.index(w)\n",
    "            tf_matrix[i][word_index] += 1 / n_words\n",
    "        \n",
    "    # Calculate Inverse Document Frequency (IDF)\n",
    "    idf = np.zeros(n_words_set)\n",
    "    for idx, w in enumerate(bow):\n",
    "        k = sum(1 for doc in corpus if w in doc)\n",
    "        idf[idx] = np.log10(n_docs / k) if k != 0 else 0\n",
    "\n",
    "    # Calculate TF-IDF matrix\n",
    "    tf_idf_matrix = tf_matrix * idf\n",
    "\n",
    "    # Find most important words in the corpus\n",
    "    flat_tf_idf = tf_idf_matrix.flatten()\n",
    "    highest_indices = np.argpartition(flat_tf_idf, -10)[-10:]\n",
    "    highest_values = [bow[i // n_docs] for i in highest_indices]\n",
    "    return highest_values\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
